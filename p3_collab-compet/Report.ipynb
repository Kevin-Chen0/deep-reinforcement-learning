{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux_NoVis/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.09000000171363354\n",
      "Score (max over agents) from episode 2: 0.09000000171363354\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.09000000171363354\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Goal:*** Based on the rubric, the agents must get an average award of +0.5 in over 100 consecutive episodes, after taking the maximum over both agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Learning Algorithm Training Description\n",
    "***Overview:***\n",
    "\n",
    "The agent uses a standard ***Multi-Agent Deep Deterministic Policy Gradient (MADDPG)*** model with two DDPG agent, each agent containing Experience Replay and fixed Q-Targets. DDPG is a deterministic and off-policy variation of an Actor-Critic method, and closely connected with to DQN. The Actors of the DDPG agents in MADDPG follow the same algo as Actor of standalone DDPG agent: taking a deterministic action that it thinks can maximize state-action value. However, what makes MADDPG quite interesting is that each and every DPPG Critic has access to ***all*** DPPG Actors' action decisions in MADDPG, known as the *centralized action-value function*, and not just the Actor within its own agent. Agents will still most likely finalize on diverse policies from each other due their differing *observations*, such as their respective locations in the env. Nevertheless, this level of omnipotency allows agent Critics to absorb policies from other agents during training, enabling agents to create *policy ensembles* leading to more robust performance in originally non-stationary environment, if the agents are blinded from each others' action. This is especially fruitful in coorperative scenarios, where the agents need to communicate and work together in order to achieve even higher scores individually. After training and during actual deployment, the agents will of course lose this omnipotency as their Actors will be operating and learning individually in the env, and can only access their own policies.\n",
    "\n",
    "Source: https://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Description:***\n",
    "\n",
    "My MADDPG contains 2 DDPG agents, the number of agents provided by the Tennis end. For each DDPG, both Actor and Critic (as well as their respective local and target networks) have two hidden layers, with first one (fc1) having 512 units and second one (fc2) having 256 units. I am additionally using Leaky ReLu rather than ReLu to prevent neurons from getting \"stuck\" due to 0 values. Finally, I have removed the OUNoise as it interferes too much MADDPG, which is already sensitive enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `BUFFER_SIZE` = 100000 (1e6)\n",
    "* `BATCH_SIZE` = 512\n",
    "* `GAMMA` = 0.99\n",
    "* `TAU` = 0.01\n",
    "* `LR_ACTOR` = 0.0002  (learning rate of actor) \n",
    "* `LR_CRITIC` = 0.0002 (learning rate of critic)\n",
    "* `WEIGHT_DECAY` = 0   (L2) \n",
    "* `UPDATE_EVERY` = 2\n",
    "* `UPDATE_AMOUNT` = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maddpg_agent import MADDPG\n",
    "agents = MADDPG(num_agents=num_agents, state_size=state_size,\n",
    "                                       action_size=action_size, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=100, max_t=1000):\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradiant.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    scores_output = []\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agents.reset()\n",
    "        scores = np.zeros(num_agents) # initialize all the agents' scores to 0\n",
    "\n",
    "        for t in range(max_t):\n",
    "            actions = agents.act(states)  # select an action\n",
    "            env_info = env.step(actions)[brain_name]  # send the action to env\n",
    "            next_states = env_info.vector_observations  # get the next state\n",
    "            rewards = env_info.rewards  # get the reward\n",
    "            done = env_info.local_done  # see if episode has finished\n",
    "            agents.step(states, actions, rewards, next_states, done)\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "        max_score = np.max(scores)\n",
    "        scores_window.append(max_score)    # save maximum score among the agents\n",
    "        scores_output.append(max_score)    # save maximum score among the agents\n",
    "        avg_score = np.mean(scores_window) # retrieve the agents' avg score\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if avg_score >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            for agent in agents.agents:\n",
    "                actor = agent.actor_local.state_dict()\n",
    "                critic = agent.critic_local.state_dict()\n",
    "                id = agent.agent_id\n",
    "                torch.save(actor, f'checkpoint_a{id}_actor.pth')\n",
    "                torch.save(critic, f'checkpoint_a{id}_critic.pth')\n",
    "            break\n",
    "\n",
    "    return scores_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00\n",
      "Episode 200\tAverage Score: 0.00\n",
      "Episode 300\tAverage Score: 0.00\n",
      "Episode 400\tAverage Score: 0.01\n",
      "Episode 500\tAverage Score: 0.05\n",
      "Episode 600\tAverage Score: 0.08\n",
      "Episode 700\tAverage Score: 0.08\n",
      "Episode 800\tAverage Score: 0.11\n",
      "Episode 900\tAverage Score: 0.13\n",
      "Episode 1000\tAverage Score: 0.15\n",
      "Episode 1100\tAverage Score: 0.15\n",
      "Episode 1200\tAverage Score: 0.15\n",
      "Episode 1300\tAverage Score: 0.19\n",
      "Episode 1400\tAverage Score: 0.19\n",
      "Episode 1500\tAverage Score: 0.16\n",
      "Episode 1600\tAverage Score: 0.26\n",
      "Episode 1700\tAverage Score: 0.35\n",
      "Episode 1800\tAverage Score: 0.31\n",
      "Episode 1900\tAverage Score: 0.29\n",
      "Episode 2000\tAverage Score: 0.43\n",
      "Episode 2100\tAverage Score: 0.38\n",
      "Episode 2200\tAverage Score: 0.46\n",
      "Episode 2212\tAverage Score: 0.51\n",
      "Environment solved in 2212 episodes!\tAverage Score: 0.51\n"
     ]
    }
   ],
   "source": [
    "scores = train(n_episodes=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot of Rewards\n",
    "***Number of Eps:*** 2212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.plot(np.arange(len(scores)), pd.DataFrame(scores).rolling(20).mean())\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ideas for Future Work\n",
    "***Performance Improvement:***\n",
    "\n",
    "1) ***Prioritized Experienced Replay***: As explained in my Navigation project, this allows my model to learn from more \"important\" experiences more frequently. Importance in terms of the absolute TD Error. Improvement on more \"important\" experiences allows for faster gradient descent off the cost function, or faster increase of the score. Experience prioritization is something I am still currently missing.\n",
    "\n",
    "2) ***Update Amount***: I think the model can converge in less episode if it learns multiple times each step. However, the trade-off is that each n-step learning interval will be slower per increment of update amount.\n",
    "\n",
    "3) ***Learning Rate***: I have intentionally set the learning rate low in order for the model to slowly yet steadily train. I can further try to tweak to rates higher, but from my experience, it leads to more inconsistent results.\n",
    "\n",
    "4) ***MAD4PG***: As explained Continuous Control project, Distributed Distributional Deep Deterministic Policy Gradient (D4PG) apparently improves upon DDPG in terms of coping with more complicated continuous environments, including this one. I assume that that also applies to MAD4PG comparison with MADDPG.\n",
    "\n",
    "5) ***PPO/TRPO***: Using the following policy-only models can potentially provide more robust performance due to their sampling efficiency and use of penalities. Could possibly use PPO or TRPO that as an Actor rather than DQN. *Source*: https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed\n",
    "\n",
    "6) ***GPU Cluster***: I am currently running it on my own CPU laptop, which is painfully slow. I should try to run it on a GPU Cluster (i.e. cloud) in order to do training in-parallel. There is where I can experiment with asynchronous models, such as A3C. \n",
    "\n",
    "7) ***Soccer***: As future work, I can test the above Performance Improvement ideas with the Soccer project, which has more agents to manage. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
